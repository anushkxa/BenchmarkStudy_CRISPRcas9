{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpkmVB7brsM1",
        "outputId": "4aeffead-9625-4b4b-f082-6c389f3ad2fd",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras) (0.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install the exact library versions from the GitHub repository\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset you just uploaded\n",
        "df = pd.read_csv('GUIDE-Seq.csv')\n",
        "\n",
        "# Display the first 5 rows to see the format\n",
        "print(\"--- First 5 rows of the dataset ---\")\n",
        "print(df.head())\n",
        "\n",
        "# Display the column names so we know what to use\n",
        "print(\"\\n--- Column Names ---\")\n",
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30M7GxhQwgoR",
        "outputId": "ed3c5000-8a74-4c7d-edf1-ee7886bb837c",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- First 5 rows of the dataset ---\n",
            "                        DNA                     crRNA  label  read  \\\n",
            "0  GCTGCCAGTACAGGCTCCCCCTCG  GCAGCCAGTACA_GCTCACCATGG    0.0   0.0   \n",
            "1  GCTGCCAGTACAGGCTCCCCCTCG  GCAGCCAGTACAG_CTCACCATGG    0.0   0.0   \n",
            "2  TACTAGAGTGACAAGTCACACAAT  G_CTAGAGTCACAAGTCCCACAGG    0.0   0.0   \n",
            "3  -C_TAGAGTGACAAGTCACACAAT  -GCTAGAGTCACAAGTCCCACAGG    0.0   0.0   \n",
            "4  ACAGCGAGTACAAGCTCATCATGA  GCAGCCAGTAC_AGCTCACCATGG    0.0   0.0   \n",
            "\n",
            "                                                pair  \n",
            "0  GCAGCCAGTACA_GCTCACCATGG|GCTGCCAGTACAGGCTCCCCCTCG  \n",
            "1  GCAGCCAGTACAG_CTCACCATGG|GCTGCCAGTACAGGCTCCCCCTCG  \n",
            "2  G_CTAGAGTCACAAGTCCCACAGG|TACTAGAGTGACAAGTCACACAAT  \n",
            "3  -GCTAGAGTCACAAGTCCCACAGG|-C_TAGAGTGACAAGTCACACAAT  \n",
            "4  GCAGCCAGTAC_AGCTCACCATGG|ACAGCGAGTACAAGCTCATCATGA  \n",
            "\n",
            "--- Column Names ---\n",
            "['DNA', 'crRNA', 'label', 'read', 'pair']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- Column names identified from your output ---\n",
        "SGRNA_COLUMN = 'crRNA'\n",
        "OFFTARGET_COLUMN = 'DNA'\n",
        "LABEL_COLUMN = 'label'\n",
        "\n",
        "# --- You shouldn't need to change anything below this line ---\n",
        "\n",
        "# Create a copy to work with\n",
        "clean_df = df.copy()\n",
        "\n",
        "# 1. Clean the sequence data by removing any character that is not A, C, G, or T\n",
        "clean_df[SGRNA_COLUMN] = clean_df[SGRNA_COLUMN].str.replace('[^ACGT]', '', regex=True)\n",
        "clean_df[OFFTARGET_COLUMN] = clean_df[OFFTARGET_COLUMN].str.replace('[^ACGT]', '', regex=True)\n",
        "\n",
        "# 2. NEW STEP: Truncate any sequences longer than 23 bp to exactly 23 bp\n",
        "clean_df[SGRNA_COLUMN] = clean_df[SGRNA_COLUMN].str[:23]\n",
        "clean_df[OFFTARGET_COLUMN] = clean_df[OFFTARGET_COLUMN].str[:23]\n",
        "\n",
        "# 3. Now, filter to keep ONLY the rows where both sequences are exactly 23 characters long\n",
        "initial_rows = len(clean_df)\n",
        "clean_df = clean_df[\n",
        "    (clean_df[SGRNA_COLUMN].str.len() == 23) &\n",
        "    (clean_df[OFFTARGET_COLUMN].str.len() == 23)\n",
        "]\n",
        "print(f\"Standardized sequence lengths. Kept {len(clean_df)} out of {initial_rows} rows.\")\n",
        "\n",
        "# 4. Select and rename the columns to the standard format\n",
        "final_df = clean_df[[SGRNA_COLUMN, OFFTARGET_COLUMN, LABEL_COLUMN]].copy()\n",
        "final_df.rename(columns={\n",
        "    SGRNA_COLUMN: 'sgRNA',\n",
        "    OFFTARGET_COLUMN: 'off_target',\n",
        "    LABEL_COLUMN: 'label'\n",
        "}, inplace=True)\n",
        "\n",
        "# 5. Ensure the label is an integer (0 or 1)\n",
        "final_df['label'] = final_df['label'].astype(int)\n",
        "\n",
        "print(\"\\n--- First 5 rows of cleaned and formatted data ---\")\n",
        "print(final_df.head())\n",
        "\n",
        "# Save the final, clean data to a new CSV file\n",
        "final_df.to_csv('cleaned_benchmark_data.csv', index=False)\n",
        "print(\"\\n✅ Success! Your benchmark dataset is ready and saved as 'cleaned_benchmark_data.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktj7CYXpxdOa",
        "outputId": "b86e87a1-5e8d-44b8-a2cb-7f6c417d7eca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized sequence lengths. Kept 149483 out of 213933 rows.\n",
            "\n",
            "--- First 5 rows of cleaned and formatted data ---\n",
            "                     sgRNA               off_target  label\n",
            "0  GCAGCCAGTACAGCTCACCATGG  GCTGCCAGTACAGGCTCCCCCTC      0\n",
            "1  GCAGCCAGTACAGCTCACCATGG  GCTGCCAGTACAGGCTCCCCCTC      0\n",
            "2  GCTAGAGTCACAAGTCCCACAGG  TACTAGAGTGACAAGTCACACAA      0\n",
            "4  GCAGCCAGTACAGCTCACCATGG  ACAGCGAGTACAAGCTCATCATG      0\n",
            "5  GCAGCCAGTACAGCTCACCATGG  ACAGCGAGTACAAGCTCATCATG      0\n",
            "\n",
            "✅ Success! Your benchmark dataset is ready and saved as 'cleaned_benchmark_data.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CnnCRISPR**"
      ],
      "metadata": {
        "id": "KSkvx7e8opd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/LQYoLH/CnnCrispr.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsicrgX7yafu",
        "outputId": "84b1ae9f-9060-4392-8256-6c4f2885e26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CnnCrispr'...\n",
            "remote: Enumerating objects: 122, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 122 (delta 10), reused 0 (delta 0), pack-reused 84 (from 1)\u001b[K\n",
            "Receiving objects: 100% (122/122), 24.81 MiB | 19.13 MiB/s, done.\n",
            "Resolving deltas: 100% (38/38), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls CnnCrispr/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDpGSZdjzFUm",
        "outputId": "5e26f43f-3311-49b9-b92d-978d17b15bb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CnnCrispr_code\t images\t\t offtarget_data.rar  test1\n",
            "CnnCrispr_final  offtarget_data  README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find any file in the repository containing 'GloVe' in its name\n",
        "!find ./CnnCrispr -name \"*GloVe*\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTGoeJLQ8J-r",
        "outputId": "939e25fb-574f-4761-d930-838e23ca057e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./CnnCrispr/CnnCrispr_final/Encoded_data/Class/keras_GloVeVec_5_100_10000.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Load benchmark dataset ---\n",
        "df = pd.read_csv('cleaned_benchmark_data.csv')\n",
        "print(f\"📄 Loaded {len(df)} samples.\")\n",
        "\n",
        "# --- 2. Load the pre-trained GloVe embeddings ---\n",
        "glove_path = 'CnnCrispr/CnnCrispr_final/Encoded_data/Class/keras_GloVeVec_5_100_10000.csv'\n",
        "glove_matrix = np.loadtxt(glove_path, delimiter=',')\n",
        "glove_dict = {int(row[0]): row[1:] for row in glove_matrix}\n",
        "print(\"✅ Loaded GloVe embeddings of shape:\", np.array(list(glove_dict.values())).shape)\n",
        "\n",
        "# --- 3. Define nucleotide-to-index mapping ---\n",
        "nu_dict = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
        "\n",
        "# --- 4. Encode sgRNA–off-target pairs using GloVe embeddings ---\n",
        "encoded_sequences = []\n",
        "labels = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    sgrna_seq = row['sgRNA'].strip()\n",
        "    offtarget_seq = row['off_target'].strip()\n",
        "    label = int(row['label'])\n",
        "\n",
        "    seq_embeddings = []\n",
        "    for i in range(23):\n",
        "        s = sgrna_seq[i]\n",
        "        o = offtarget_seq[i]\n",
        "        pair_index = nu_dict[s] * 4 + nu_dict[o]  # 0–15 mapping\n",
        "        seq_embeddings.append(glove_dict[pair_index])\n",
        "\n",
        "    encoded_sequences.append(seq_embeddings)\n",
        "    labels.append(label)\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "X = np.array(encoded_sequences)  # shape: (N, 23, 100)\n",
        "y = np.array(labels)\n",
        "print(\"✅ Final encoded data shape:\", X.shape)\n",
        "\n",
        "# --- 5. Save for prediction ---\n",
        "np.savez('cnncrispr_benchmark_encoded.npz', X=X, y=y)\n",
        "print(\"💾 Saved preprocessed benchmark data for CNNCRISPR.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxQNZ-149xaH",
        "outputId": "66b22c61-ebb0-4c49-95fd-8aa07cb9310c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Loaded 149483 samples.\n",
            "✅ Loaded GloVe embeddings of shape: (16, 100)\n",
            "✅ Final encoded data shape: (149483, 23, 100)\n",
            "💾 Saved preprocessed benchmark data for CNNCRISPR.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "====================================================================\n",
        "CRITICAL: RESTART YOUR KERNEL BEFORE RUNNING THIS CODE!\n",
        "====================================================================\n",
        "\n",
        "In Google Colab: Runtime -> Restart runtime (then run all cells)\n",
        "In Jupyter: Kernel -> Restart (then run all cells)\n",
        "\n",
        "This ensures TensorFlow starts in eager execution mode from the beginning.\n",
        "====================================================================\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "import tensorflow as tf\n",
        "\n",
        "# Verify eager execution\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Eager execution enabled: {tf.executing_eagerly()}\")\n",
        "\n",
        "if not tf.executing_eagerly():\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"❌ ERROR: Eager execution is NOT enabled!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nYou MUST restart your kernel/runtime before running this code.\")\n",
        "    print(\"\\nIn Google Colab: Runtime -> Restart runtime\")\n",
        "    print(\"In Jupyter: Kernel -> Restart\")\n",
        "    print(\"\\nThen run this cell again.\")\n",
        "    print(\"=\"*70)\n",
        "    raise RuntimeError(\"Please restart kernel and try again\")\n",
        "\n",
        "print(\"✅ Eager execution is enabled. Proceeding...\\n\")\n",
        "\n",
        "# --- 1. Perform simple integer encoding ---\n",
        "print(\"Step 1: Loading and encoding data...\")\n",
        "df = pd.read_csv('cleaned_benchmark_data.csv')\n",
        "nu_dict = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
        "encoded_vectors, labels = [], []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    sgrna_seq, offtarget_seq = row['sgRNA'], row['off_target']\n",
        "    label, vector = row['label'], []\n",
        "\n",
        "    for i in range(23):\n",
        "        try:\n",
        "            s_nuc, o_nuc = sgrna_seq[i], offtarget_seq[i]\n",
        "            vector.append(nu_dict[s_nuc] * 4 + nu_dict[o_nuc])\n",
        "        except KeyError:\n",
        "            vector = []\n",
        "            break\n",
        "\n",
        "    if len(vector) == 23:\n",
        "        encoded_vectors.append(vector)\n",
        "        labels.append(label)\n",
        "\n",
        "X_test = np.array(encoded_vectors, dtype=np.int32)\n",
        "y_test = np.array(labels)\n",
        "\n",
        "print(f\"✅ Data loaded: {X_test.shape}\\n\")\n",
        "\n",
        "# --- 2. Build and load model ---\n",
        "print(\"Step 2: Building model architecture...\")\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Conv1D, BatchNormalization, Flatten, Dense, Dropout, Input\n",
        "\n",
        "inputs = Input(shape=(23,), dtype='int32')\n",
        "x = Embedding(input_dim=16, output_dim=100)(inputs)\n",
        "x = Bidirectional(LSTM(40, return_sequences=True))(x)\n",
        "x = Conv1D(10, 5, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv1D(20, 5, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv1D(40, 5, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv1D(80, 5, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv1D(100, 5, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Flatten()(x)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Dense(20, activation='relu')(x)\n",
        "outputs = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "print(\"Step 3: Loading model weights...\")\n",
        "weights_path = 'CnnCrispr/CnnCrispr_final/Model_save/CnnCrispr_weights.h5'\n",
        "model.load_weights(weights_path)\n",
        "print(\"✅ Model loaded successfully\\n\")\n",
        "\n",
        "# --- 3. Make predictions in eager mode ---\n",
        "print(\"Step 4: Making predictions...\")\n",
        "print(f\"Processing {len(X_test)} samples in batches...\\n\")\n",
        "\n",
        "batch_size = 256\n",
        "all_predictions = []\n",
        "\n",
        "for i in range(0, len(X_test), batch_size):\n",
        "    batch = X_test[i:i+batch_size]\n",
        "    batch_pred = model(batch, training=False).numpy()\n",
        "    all_predictions.append(batch_pred)\n",
        "\n",
        "    if (i // batch_size + 1) % 50 == 0:\n",
        "        print(f\"Processed {min(i+batch_size, len(X_test))}/{len(X_test)} samples\")\n",
        "\n",
        "y_pred_probs = np.vstack(all_predictions)\n",
        "y_scores = y_pred_probs[:, 1]\n",
        "\n",
        "print(f\"\\n✅ Predictions completed! Shape: {y_pred_probs.shape}\\n\")\n",
        "\n",
        "# --- 4. Calculate and display performance ---\n",
        "auroc = roc_auc_score(y_test, y_scores)\n",
        "auprc = average_precision_score(y_test, y_scores)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"🎯 CnnCRISPR Performance Results\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Area Under ROC Curve (AUROC): {auroc:.4f}\")\n",
        "print(f\"Area Under PR Curve (AUPRC):  {auprc:.4f}\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmAJPjDYG_Eo",
        "outputId": "59295e0e-4c86-46cb-c440-5cfa22d1baf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "Eager execution enabled: True\n",
            "✅ Eager execution is enabled. Proceeding...\n",
            "\n",
            "Step 1: Loading and encoding data...\n",
            "✅ Data loaded: (149483, 23)\n",
            "\n",
            "Step 2: Building model architecture...\n",
            "Step 3: Loading model weights...\n",
            "✅ Model loaded successfully\n",
            "\n",
            "Step 4: Making predictions...\n",
            "Processing 149483 samples in batches...\n",
            "\n",
            "Processed 12800/149483 samples\n",
            "Processed 25600/149483 samples\n",
            "Processed 38400/149483 samples\n",
            "Processed 51200/149483 samples\n",
            "Processed 64000/149483 samples\n",
            "Processed 76800/149483 samples\n",
            "Processed 89600/149483 samples\n",
            "Processed 102400/149483 samples\n",
            "Processed 115200/149483 samples\n",
            "Processed 128000/149483 samples\n",
            "Processed 140800/149483 samples\n",
            "\n",
            "✅ Predictions completed! Shape: (149483, 2)\n",
            "\n",
            "======================================================================\n",
            "🎯 CnnCRISPR Performance Results\n",
            "======================================================================\n",
            "Area Under ROC Curve (AUROC): 0.8871\n",
            "Area Under PR Curve (AUPRC):  0.0050\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PI-CRISPR**"
      ],
      "metadata": {
        "id": "5ENNr1F1ifrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/florianst/picrispr.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGKZvquKimSd",
        "outputId": "fff7d5fc-0a27-4189-ee09-ef9bfcbe4899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'picrispr'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 61 (delta 5), reused 10 (delta 3), pack-reused 46 (from 1)\u001b[K\n",
            "Receiving objects: 100% (61/61), 58.65 MiB | 26.38 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n",
            "Updating files: 100% (42/42), done.\n",
            "Filtering content: 100% (2/2), 1000.32 MiB | 51.15 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install tensorflow\n",
        "!pip install torch\n",
        "!pip install xgboost\n",
        "!pip install scikit-learn pandas numpy matplotlib scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9ZRdYJ4Riy29",
        "outputId": "d6401d3e-5473-4cab-9aa0-1aad65a5733d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -R picrispr/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Hyph4jvrkURx",
        "outputId": "05bf181e-c02f-4d8e-e37d-d0d155f34e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "picrispr/:\n",
            "default_vals  models\t\t\t    picrispr.py       test_input.csv\n",
            "encoding.py   models.py\t\t\t    README.md\n",
            "load_data.py  offtarget_260520_nuc.csv.zip  requirements.txt\n",
            "\n",
            "picrispr/default_vals:\n",
            "defaultvals_tf_interface_type_s2_class.pickle\n",
            "defaultvals_tf_interface_type_s2.pickle\n",
            "defaultvals_tf_interface_type_s4_class.pickle\n",
            "defaultvals_tf_interface_type_s4.pickle\n",
            "defaultvals_tf_s2_class.pickle\n",
            "defaultvals_tf_s2.pickle\n",
            "defaultvals_tf_s4_class.pickle\n",
            "defaultvals_tf_s4.pickle\n",
            "defaultvals_torch_interface_type_s2_class.pickle\n",
            "defaultvals_torch_interface_type_s2.pickle\n",
            "defaultvals_torch_interface_type_s4_class.pickle\n",
            "defaultvals_torch_interface_type_s4.pickle\n",
            "defaultvals_torch_s2_class.pickle\n",
            "defaultvals_torch_s2.pickle\n",
            "defaultvals_torch_s4_class.pickle\n",
            "defaultvals_torch_s4.pickle\n",
            "\n",
            "picrispr/models:\n",
            "models_torch.zip\n",
            "trainresult_tf_interface_type_s2_class.pickle\n",
            "trainresult_tf_interface_type_s2_class_weights.pickle\n",
            "trainresult_tf_interface_type_s2.pickle\n",
            "trainresult_tf_interface_type_s2_weights.pickle\n",
            "trainresult_tf_interface_type_s4_class.pickle\n",
            "trainresult_tf_interface_type_s4_class_weights.pickle\n",
            "trainresult_tf_interface_type_s4.pickle\n",
            "trainresult_tf_interface_type_s4_weights.pickle\n",
            "trainresult_tf_s2_class.pickle\n",
            "trainresult_tf_s2_class_weights.pickle\n",
            "trainresult_tf_s2.pickle\n",
            "trainresult_tf_s2_weights.pickle\n",
            "trainresult_tf_s4_class.pickle\n",
            "trainresult_tf_s4_class_weights.pickle\n",
            "trainresult_tf_s4.pickle\n",
            "trainresult_tf_s4_weights.pickle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the pre-trained models into the 'models' directory\n",
        "!unzip -q picrispr/models/models_torch.zip -d picrispr/models/\n",
        "print(\"✅ Pre-trained models unzipped successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vge-yx-2l5xk",
        "outputId": "84407fc6-ca59-44c3-d5d4-dbb4a7e1b58e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Pre-trained models unzipped successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('cleaned_benchmark_data.csv')\n",
        "predict_only_df = df.rename(columns={\n",
        "    'sgRNA': 'grna_target_sequence',\n",
        "    'off_target': 'target_sequence'\n",
        "}).drop(columns=['label'])\n",
        "predict_only_df.to_csv('picrispr_predict_only.csv', index=False)\n",
        "print(\"✅ Created 'picrispr_predict_only.csv'\")\n",
        "\n",
        "# === STEP 5: RUN PREDICTION ===\n",
        "print(\"\\n--- Running piCRISPR prediction ---\")\n",
        "# Using model 2 (RNN 6x23 nuc), the best performer from the paper\n",
        "!python picrispr/picrispr.py picrispr_predict_only.csv 2 picrispr/models False\n",
        "\n",
        "# === STEP 6: CALCULATE AND DISPLAY RESULTS ===\n",
        "print(\"\\n--- Calculating final performance metrics ---\")\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "try:\n",
        "    results_df = pd.read_csv('output.csv')\n",
        "    ground_truth_df = pd.read_csv('cleaned_benchmark_data.csv')\n",
        "\n",
        "    y_test = ground_truth_df['label']\n",
        "    y_scores = results_df['piCRISPR prediction']\n",
        "\n",
        "    auroc = roc_auc_score(y_test, y_scores)\n",
        "    auprc = average_precision_score(y_test, y_scores)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"🎯 piCRISPR Performance Results on Your Benchmark Dataset\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Area Under ROC Curve (AUROC): {auroc:.4f}\")\n",
        "    print(f\"Area Under PR Curve (AUPRC):  {auprc:.4f}\")\n",
        "    print(\"=\"*70)\n",
        "except FileNotFoundError:\n",
        "    print(\"❌ Error: 'output.csv' was not created. Please review the prediction logs above for errors.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during evaluation: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ52q2GKoEir",
        "outputId": "6f74c787-5f0f-4422-f0d7-3dc673170784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Created 'picrispr_predict_only.csv'\n",
            "\n",
            "--- Running piCRISPR prediction ---\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760360243.149045    1707 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760360243.159426    1707 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760360243.194651    1707 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760360243.194692    1707 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760360243.194701    1707 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760360243.194708    1707 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "tf _s4 __________________________________\n",
            "encoding dataset...WARNING: Feature NucleotideBDM was not given and no default values could be found. It will be filled with zeros, which will likely lead to decreased prediction accuracy.\n",
            "WARNING: Feature NuPoP_Affinity_147_human was not given and no default values could be found. It will be filled with zeros, which will likely lead to decreased prediction accuracy.\n",
            "WARNING: Feature GCContent was not given and no default values could be found. It will be filled with zeros, which will likely lead to decreased prediction accuracy.\n",
            "WARNING: Feature energy_2+energy_1-(energy_3*energy_4/energy_2) was not given and no default values could be found. It will be filled with zeros, which will likely lead to decreased prediction accuracy.\n",
            "/content/picrispr/picrispr.py:92: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['cleavage_freq'].fillna(0, inplace=True)\n",
            "149483it [00:05, 28963.90it/s]\n",
            "0it [00:00, ?it/s]\n",
            "/content/picrispr/picrispr.py:588: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:653.)\n",
            "  return sparse_tensortype(indices, values, x.size())\n",
            "using GPU device Tesla T4\n",
            "done\n",
            "I0000 00:00:1760360260.863172    1707 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "I0000 00:00:1760360264.589471    1707 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
            "loading model...done\n",
            "preparing dataset...done\n",
            "obtaining predictions...\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m535s\u001b[0m 286ms/step\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m532s\u001b[0m 284ms/step\n",
            "\u001b[1m922/922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 283ms/step\n",
            "100% 3/3 [22:13<00:00, 444.45s/it]\n",
            "successfully saved predictions to output.csv\n",
            "\n",
            "--- Calculating final performance metrics ---\n",
            "\n",
            "======================================================================\n",
            "🎯 piCRISPR Performance Results on Your Benchmark Dataset\n",
            "======================================================================\n",
            "Area Under ROC Curve (AUROC): 0.8842\n",
            "Area Under PR Curve (AUPRC):  0.0020\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **deep crispr**"
      ],
      "metadata": {
        "id": "hPL-PuTYdkEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === STEP 1: Set Up the DeepCRISPR Environment (TF2 Compatibility) ===\n",
        "\n",
        "# Install modern, compatible libraries\n",
        "!pip install tensorflow\n",
        "!pip install dm-sonnet\n",
        "!pip install pandas scikit-learn\n",
        "\n",
        "print(\"\\n✅ Modern environment for DeepCRISPR is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CHRlTL8DdseD",
        "outputId": "77b2e0f3-c89a-43db-94cc-186decc42acd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Collecting dm-sonnet\n",
            "  Downloading dm_sonnet-2.0.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from dm-sonnet) (1.4.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from dm-sonnet) (0.1.9)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.12/dist-packages (from dm-sonnet) (2.0.2)\n",
            "Requirement already satisfied: tabulate>=0.7.5 in /usr/local/lib/python3.12/dist-packages (from dm-sonnet) (0.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from dm-sonnet) (1.17.3)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.12/dist-packages (from dm-tree>=0.1.1->dm-sonnet) (25.4.0)\n",
            "Downloading dm_sonnet-2.0.2-py3-none-any.whl (268 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.4/268.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dm-sonnet\n",
            "Successfully installed dm-sonnet-2.0.2\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "\n",
            "✅ Modern environment for DeepCRISPR is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Step 1: Loading your benchmark dataset...\")\n",
        "df = pd.read_csv('cleaned_benchmark_data.csv')\n",
        "\n",
        "# --- Define the encoding logic for DeepCRISPR ---\n",
        "nuc_map = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
        "# The model expects input shape: [channels, 1, sequence_length]\n",
        "# For sequence-only, channels = 4 and sequence_length = 23\n",
        "channels = 4\n",
        "seq_len = 23\n",
        "\n",
        "def one_hot_encode(sequence):\n",
        "    \"\"\"Encodes a 23-bp DNA sequence into the 4x1x23 format for DeepCRISPR.\"\"\"\n",
        "    # Initialize a matrix of zeros with the required shape\n",
        "    encoded_matrix = np.zeros((channels, 1, seq_len), dtype=np.uint8)\n",
        "\n",
        "    for i, nucleotide in enumerate(sequence):\n",
        "        if nucleotide in nuc_map:\n",
        "            channel_index = nuc_map[nucleotide]\n",
        "            encoded_matrix[channel_index, 0, i] = 1\n",
        "    return encoded_matrix\n",
        "\n",
        "# Lists to store the processed data\n",
        "sg_features = []\n",
        "ot_features = []\n",
        "labels = []\n",
        "\n",
        "print(\"Step 2: Encoding data into the two-part, 4-channel format...\")\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    sgRNA_seq = row['sgRNA']\n",
        "    off_target_seq = row['off_target']\n",
        "    label = row['label']\n",
        "\n",
        "    # Encode both sequences\n",
        "    sg_encoded = one_hot_encode(sgRNA_seq)\n",
        "    ot_encoded = one_hot_encode(off_target_seq)\n",
        "\n",
        "    # Add the encoded matrices and the label to our lists\n",
        "    sg_features.append(sg_encoded)\n",
        "    ot_features.append(ot_encoded)\n",
        "    labels.append(label)\n",
        "\n",
        "# Convert lists to final NumPy arrays\n",
        "X_sg = np.array(sg_features)\n",
        "X_ot = np.array(ot_features)\n",
        "y = np.array(labels)\n",
        "\n",
        "print(\"\\n✅ Preprocessing complete!\")\n",
        "print(f\"Shape of sgRNA features: {X_sg.shape}\")\n",
        "print(f\"Shape of Off-target features: {X_ot.shape}\")\n",
        "print(f\"Shape of Labels: {y.shape}\")\n",
        "\n",
        "# Save the preprocessed data into a single, organized file\n",
        "np.savez(\n",
        "    'deepcrispr_benchmark_encoded.npz',\n",
        "    X_sg=X_sg,\n",
        "    X_ot=X_ot,\n",
        "    y=y\n",
        ")\n",
        "print(\"\\n💾 Saved preprocessed data to 'deepcrispr_benchmark_encoded.npz'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwAhyZA5efAD",
        "outputId": "816eece0-3878-4a25-cdb3-c7a1d847bde4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Loading your benchmark dataset...\n",
            "Step 2: Encoding data into the two-part, 4-channel format...\n",
            "\n",
            "✅ Preprocessing complete!\n",
            "Shape of sgRNA features: (149483, 4, 1, 23)\n",
            "Shape of Off-target features: (149483, 4, 1, 23)\n",
            "Shape of Labels: (149483,)\n",
            "\n",
            "💾 Saved preprocessed data to 'deepcrispr_benchmark_encoded.npz'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the old directory to ensure a clean start\n",
        "!rm -rf DeepCRISPR/trained_models/offtar_pt_cnn\n",
        "\n",
        "# Re-create the directory\n",
        "!mkdir -p DeepCRISPR/trained_models/offtar_pt_cnn\n",
        "\n",
        "# Unpack the model files into the new directory\n",
        "!tar -xzf DeepCRISPR/trained_models/offtar_pt_cnn.tar.gz -C DeepCRISPR/trained_models/offtar_pt_cnn/\n",
        "\n",
        "print(\"✅ Pre-trained off-target model unpacked successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iahhKadwf76R",
        "outputId": "8c95f6aa-47f1-47bc-82c4-dcfcf2c24928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar (child): DeepCRISPR/trained_models/offtar_pt_cnn.tar.gz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n",
            "✅ Pre-trained off-target model unpacked successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "# === STEP 1: INSTALL LIBRARIES ===\n",
        "print(\"--- Installing libraries ---\")\n",
        "# Use a compatible TensorFlow version\n",
        "!pip install tensorflow scikit-learn pandas -q\n",
        "print(\"✅ Libraries installed.\\n\")\n",
        "\n",
        "# === STEP 2: CLONE REPOSITORY ===\n",
        "print(\"--- Cloning DeepCRISPR repository ---\")\n",
        "if not os.path.exists('DeepCRISPR'):\n",
        "    !git clone https://github.com/bm2-lab/DeepCRISPR.git\n",
        "    print(\"✅ Repository cloned.\")\n",
        "else:\n",
        "    print(\"✅ Repository already exists.\")\n",
        "print()\n",
        "\n",
        "# === STEP 3: CHECK AND LIST AVAILABLE MODEL FILES ===\n",
        "print(\"--- Checking available model files ---\")\n",
        "trained_models_dir = 'DeepCRISPR/trained_models/'\n",
        "\n",
        "if not os.path.exists(trained_models_dir):\n",
        "    print(f\"❌ Error: Directory '{trained_models_dir}' not found!\")\n",
        "    print(\"Please check if the repository was cloned correctly.\")\n",
        "else:\n",
        "    print(f\"Contents of {trained_models_dir}:\")\n",
        "    for item in os.listdir(trained_models_dir):\n",
        "        full_path = os.path.join(trained_models_dir, item)\n",
        "        if os.path.isfile(full_path):\n",
        "            size = os.path.getsize(full_path)\n",
        "            print(f\"  - {item} ({size/1024:.1f} KB)\")\n",
        "        else:\n",
        "            print(f\"  - {item}/ (directory)\")\n",
        "    print()\n",
        "\n",
        "# === STEP 4: FIND AND UNPACK THE MODEL ===\n",
        "print(\"--- Finding and unpacking the pre-trained model ---\")\n",
        "\n",
        "# Try to find the correct archive file\n",
        "possible_archives = [\n",
        "    'DeepCRISPR/trained_models/offtar_pt_cnn.tar.gz',\n",
        "    'DeepCRISPR/trained_models/off_target_pt_cnn.tar.gz',\n",
        "    'DeepCRISPR/trained_models/offtarget_pt_cnn.tar.gz'\n",
        "]\n",
        "\n",
        "archive_path = None\n",
        "for path in possible_archives:\n",
        "    if os.path.exists(path):\n",
        "        archive_path = path\n",
        "        print(f\"✅ Found model archive: {path}\")\n",
        "        break\n",
        "\n",
        "if archive_path is None:\n",
        "    # List all .tar.gz files\n",
        "    print(\"\\n⚠️ Standard model file not found. Searching for any .tar.gz files...\")\n",
        "    tar_gz_files = []\n",
        "    for root, dirs, files in os.walk(trained_models_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.tar.gz'):\n",
        "                tar_gz_files.append(os.path.join(root, file))\n",
        "\n",
        "    if tar_gz_files:\n",
        "        print(\"\\nFound the following .tar.gz files:\")\n",
        "        for i, f in enumerate(tar_gz_files):\n",
        "            print(f\"  {i+1}. {f}\")\n",
        "        archive_path = tar_gz_files[0]\n",
        "        print(f\"\\n✅ Using: {archive_path}\")\n",
        "    else:\n",
        "        print(\"\\n❌ No .tar.gz model files found!\")\n",
        "        print(\"Please download the pre-trained model manually.\")\n",
        "        print(\"Check: https://github.com/bm2-lab/DeepCRISPR/tree/master/trained_models\")\n",
        "        exit()\n",
        "\n",
        "# Unpack the model\n",
        "model_dir = 'DeepCRISPR/trained_models/model_unpacked/'\n",
        "print(f\"\\nUnpacking model to: {model_dir}\")\n",
        "!rm -rf {model_dir}\n",
        "!mkdir -p {model_dir}\n",
        "\n",
        "try:\n",
        "    with tarfile.open(archive_path, 'r:gz') as tar:\n",
        "        tar.extractall(path=model_dir)\n",
        "    print(\"✅ Model unpacked successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error unpacking model: {e}\")\n",
        "    exit()\n",
        "\n",
        "# List contents of unpacked directory\n",
        "print(f\"\\nContents of {model_dir}:\")\n",
        "for item in os.listdir(model_dir):\n",
        "    print(f\"  - {item}\")\n",
        "print()\n",
        "\n",
        "# === STEP 5: FIND THE .META FILE ===\n",
        "print(\"--- Finding the model's checkpoint files ---\")\n",
        "meta_files = []\n",
        "for root, dirs, files in os.walk(model_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.meta'):\n",
        "            meta_files.append(os.path.join(root, file))\n",
        "\n",
        "if not meta_files:\n",
        "    print(f\"❌ No .meta file found in {model_dir}\")\n",
        "    print(\"\\nAll files in the directory:\")\n",
        "    for root, dirs, files in os.walk(model_dir):\n",
        "        for file in files:\n",
        "            print(f\"  - {os.path.join(root, file)}\")\n",
        "    exit()\n",
        "\n",
        "model_meta_path = meta_files[0]\n",
        "model_checkpoint_dir = os.path.dirname(model_meta_path)\n",
        "\n",
        "print(f\"✅ Found meta file: {model_meta_path}\")\n",
        "print(f\"✅ Checkpoint directory: {model_checkpoint_dir}\\n\")\n",
        "\n",
        "# === STEP 6: PREPARE THE BENCHMARK DATA ===\n",
        "print(\"--- Preparing benchmark data ---\")\n",
        "\n",
        "if not os.path.exists('cleaned_benchmark_data.csv'):\n",
        "    print(\"❌ Error: 'cleaned_benchmark_data.csv' not found!\")\n",
        "    print(\"Please upload your benchmark dataset.\")\n",
        "    exit()\n",
        "\n",
        "df = pd.read_csv('cleaned_benchmark_data.csv')\n",
        "print(f\"Loaded {len(df)} samples from benchmark data\")\n",
        "\n",
        "nuc_map = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
        "\n",
        "# Model expects shape: (batch, 1, 23, 8)\n",
        "# The 8 likely represents [sgRNA_base * 4 + offtarget_base]\n",
        "def encode_pair(sg_seq, ot_seq):\n",
        "    \"\"\"\n",
        "    Encode sgRNA-offTarget pair in the format expected by DeepCRISPR.\n",
        "    Shape: (1, 23, 8) where 8 = 4 one-hot channels for each of sg and ot\n",
        "    \"\"\"\n",
        "    encoded = np.zeros((1, 23, 8), dtype=np.float32)\n",
        "\n",
        "    for i in range(min(23, len(sg_seq), len(ot_seq))):\n",
        "        sg_nuc = sg_seq[i].upper()\n",
        "        ot_nuc = ot_seq[i].upper()\n",
        "\n",
        "        # First 4 channels: sgRNA one-hot encoding\n",
        "        if sg_nuc in nuc_map:\n",
        "            encoded[0, i, nuc_map[sg_nuc]] = 1\n",
        "\n",
        "        # Last 4 channels: off-target one-hot encoding\n",
        "        if ot_nuc in nuc_map:\n",
        "            encoded[0, i, 4 + nuc_map[ot_nuc]] = 1\n",
        "\n",
        "    return encoded\n",
        "\n",
        "# Prepare features\n",
        "sg_features, ot_features, labels = [], [], []\n",
        "\n",
        "print(\"Encoding sequences...\")\n",
        "for idx, row in df.iterrows():\n",
        "    encoded = encode_pair(row['sgRNA'], row['off_target'])\n",
        "    # DeepCRISPR likely uses the same input for both placeholders\n",
        "    sg_features.append(encoded)\n",
        "    ot_features.append(encoded)\n",
        "    labels.append(row['label'])\n",
        "\n",
        "    if (idx + 1) % 10000 == 0:\n",
        "        print(f\"  Encoded {idx + 1}/{len(df)} samples\")\n",
        "\n",
        "X_sg = np.array(sg_features, dtype=np.float32)\n",
        "X_ot = np.array(ot_features, dtype=np.float32)\n",
        "y_test = np.array(labels)\n",
        "\n",
        "print(f\"\\n✅ Data prepared successfully\")\n",
        "print(f\"   Input shape: {X_sg.shape}\")\n",
        "print(f\"   Labels shape: {y_test.shape}\")\n",
        "print(f\"   Expected model input shape: (batch, 1, 23, 8)\")\n",
        "print(f\"   Our shape matches: {X_sg.shape[1:] == (1, 23, 8)}\\n\")\n",
        "\n",
        "# === STEP 7: LOAD MODEL AND PREDICT ===\n",
        "print(\"--- Loading model and running prediction ---\")\n",
        "\n",
        "# Disable eager execution for TF1 compatibility\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    sess = tf.compat.v1.Session()\n",
        "\n",
        "    # Load the model\n",
        "    print(\"Loading model checkpoint...\")\n",
        "    saver = tf.compat.v1.train.import_meta_graph(model_meta_path)\n",
        "    saver.restore(sess, tf.train.latest_checkpoint(model_checkpoint_dir))\n",
        "    print(\"✅ Model loaded successfully\")\n",
        "\n",
        "    # Get input and output tensors\n",
        "    print(\"\\nSearching for input/output tensors...\")\n",
        "\n",
        "    # Try common tensor names\n",
        "    possible_sg_names = ['sg_input:0', 'sgRNA_input:0', 'input_1:0', 'Placeholder:0']\n",
        "    possible_ot_names = ['ot_input:0', 'offtarget_input:0', 'input_2:0', 'Placeholder_1:0']\n",
        "    possible_pred_names = ['prediction:0', 'output:0', 'Softmax:0', 'predictions:0']\n",
        "    possible_training_names = ['is_training:0', 'training:0', 'Placeholder_2:0', 'Placeholder_3:0', 'dropout:0']\n",
        "\n",
        "    x_sg_tensor = None\n",
        "    x_ot_tensor = None\n",
        "    prediction_tensor = None\n",
        "    training_tensor = None\n",
        "\n",
        "    # Try to find tensors\n",
        "    for name in possible_sg_names:\n",
        "        try:\n",
        "            x_sg_tensor = graph.get_tensor_by_name(name)\n",
        "            print(f\"✅ Found sgRNA input: {name}\")\n",
        "            break\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    for name in possible_ot_names:\n",
        "        try:\n",
        "            x_ot_tensor = graph.get_tensor_by_name(name)\n",
        "            print(f\"✅ Found off-target input: {name}\")\n",
        "            break\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    for name in possible_pred_names:\n",
        "        try:\n",
        "            prediction_tensor = graph.get_tensor_by_name(name)\n",
        "            print(f\"✅ Found prediction output: {name}\")\n",
        "            break\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Try to find training/dropout placeholder\n",
        "    training_placeholders = []\n",
        "\n",
        "    # Check all numbered placeholders\n",
        "    for i in range(2, 20):  # Check Placeholder_2 through Placeholder_19\n",
        "        name = f'Placeholder_{i}:0'\n",
        "        try:\n",
        "            tensor = graph.get_tensor_by_name(name)\n",
        "            training_placeholders.append((name, tensor))\n",
        "            print(f\"✅ Found placeholder: {name} (dtype: {tensor.dtype})\")\n",
        "        except KeyError:\n",
        "            # This placeholder doesn't exist, stop searching\n",
        "            if i > 5:  # Only break after checking at least a few\n",
        "                break\n",
        "        except Exception as e:\n",
        "            print(f\"  Error checking {name}: {e}\")\n",
        "\n",
        "    # Also try common names\n",
        "    for name in ['is_training:0', 'training:0', 'dropout:0', 'keep_prob:0']:\n",
        "        try:\n",
        "            tensor = graph.get_tensor_by_name(name)\n",
        "            if (name, tensor) not in training_placeholders:\n",
        "                training_placeholders.append((name, tensor))\n",
        "                print(f\"✅ Found training mode placeholder: {name}\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    if not training_placeholders:\n",
        "        print(\"⚠️ No training mode placeholders found\")\n",
        "\n",
        "    # If not found, list all available tensors\n",
        "    if x_sg_tensor is None or x_ot_tensor is None or prediction_tensor is None:\n",
        "        print(\"\\n⚠️ Could not find expected tensors. Listing all tensors in the graph:\")\n",
        "        all_tensors = [n.name for n in graph.as_graph_def().node]\n",
        "        for tensor_name in sorted(all_tensors)[:50]:  # Show first 50\n",
        "            print(f\"  - {tensor_name}\")\n",
        "        print(f\"\\n(Showing first 50 of {len(all_tensors)} total tensors)\")\n",
        "\n",
        "        print(\"\\n❌ Could not automatically identify the correct tensor names.\")\n",
        "        print(\"Please check the model architecture and update the tensor names manually.\")\n",
        "        sess.close()\n",
        "        exit()\n",
        "\n",
        "    # Run prediction\n",
        "    print(\"\\nRunning predictions...\")\n",
        "\n",
        "    # Build feed dict\n",
        "    feed_dict = {\n",
        "        x_sg_tensor: X_sg,\n",
        "        x_ot_tensor: X_ot\n",
        "    }\n",
        "\n",
        "    # Add all training placeholders\n",
        "    if training_placeholders:\n",
        "        print(f\"  Setting {len(training_placeholders)} placeholder(s) for inference mode\")\n",
        "        for name, tensor in training_placeholders:\n",
        "            # Check the shape and dtype to provide the right value\n",
        "            try:\n",
        "                shape = tensor.get_shape().as_list()\n",
        "            except ValueError:\n",
        "                # Unknown shape - assume scalar\n",
        "                shape = []\n",
        "\n",
        "            dtype = tensor.dtype\n",
        "\n",
        "            value_set = False\n",
        "\n",
        "            # Determine if this is a scalar or array placeholder\n",
        "            is_scalar = (shape == [] or len(shape) == 0)\n",
        "            needs_batch = (len(shape) > 0 and shape[0] is None) or (shape == [None])\n",
        "\n",
        "            if dtype == tf.bool:\n",
        "                # Boolean placeholder\n",
        "                if is_scalar:\n",
        "                    feed_dict[tensor] = False\n",
        "                elif needs_batch:\n",
        "                    feed_dict[tensor] = np.array([False] * X_sg.shape[0], dtype=bool)\n",
        "                else:\n",
        "                    feed_dict[tensor] = np.array([False] * X_sg.shape[0], dtype=bool)\n",
        "                value_set = True\n",
        "            elif dtype == tf.float32 or dtype == tf.float64:\n",
        "                # Float placeholder (like keep_prob)\n",
        "                if is_scalar:\n",
        "                    feed_dict[tensor] = 1.0  # No dropout during inference\n",
        "                elif needs_batch:\n",
        "                    feed_dict[tensor] = np.ones(X_sg.shape[0], dtype=np.float32)\n",
        "                else:\n",
        "                    feed_dict[tensor] = np.ones(X_sg.shape[0], dtype=np.float32)\n",
        "                value_set = True\n",
        "            elif dtype == tf.uint8 or dtype == tf.int32 or dtype == tf.int64:\n",
        "                # Integer placeholder - likely a flag or index\n",
        "                if is_scalar:\n",
        "                    feed_dict[tensor] = 0\n",
        "                elif needs_batch:\n",
        "                    feed_dict[tensor] = np.zeros(X_sg.shape[0], dtype=np.uint8 if dtype == tf.uint8 else np.int32)\n",
        "                else:\n",
        "                    feed_dict[tensor] = np.zeros(X_sg.shape[0], dtype=np.uint8 if dtype == tf.uint8 else np.int32)\n",
        "                value_set = True\n",
        "\n",
        "            if value_set:\n",
        "                print(f\"    {name}: shape={shape}, dtype={dtype}, value_shape={np.array(feed_dict[tensor]).shape}\")\n",
        "            else:\n",
        "                print(f\"    {name}: shape={shape}, dtype={dtype} - SKIPPED (unhandled type)\")\n",
        "\n",
        "    y_pred_probs = sess.run(prediction_tensor, feed_dict=feed_dict)\n",
        "\n",
        "    # Handle different output formats\n",
        "    if y_pred_probs.shape[-1] == 2:\n",
        "        y_scores = y_pred_probs[:, 1]\n",
        "    elif y_pred_probs.shape[-1] == 1:\n",
        "        y_scores = y_pred_probs[:, 0]\n",
        "    else:\n",
        "        y_scores = y_pred_probs.flatten()\n",
        "\n",
        "    print(f\"✅ Predictions completed. Shape: {y_pred_probs.shape}\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    auroc = roc_auc_score(y_test, y_scores)\n",
        "    auprc = average_precision_score(y_test, y_scores)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"🎯 DeepCRISPR Performance Results on Your Benchmark Dataset\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Area Under ROC Curve (AUROC): {auroc:.4f}\")\n",
        "    print(f\"Area Under PR Curve (AUPRC):  {auprc:.4f}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    sess.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Sk_8p6AZqlYr",
        "outputId": "f10214fd-8640-45a0-8cf6-0985538b6303"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Installing libraries ---\n",
            "✅ Libraries installed.\n",
            "\n",
            "--- Cloning DeepCRISPR repository ---\n",
            "✅ Repository already exists.\n",
            "\n",
            "--- Checking available model files ---\n",
            "Contents of DeepCRISPR/trained_models/:\n",
            "  - offtar_pt_cnn.tar.gz (25819.8 KB)\n",
            "  - model_unpacked/ (directory)\n",
            "  - offtar_pt_cnn_reg.tar.gz (28476.8 KB)\n",
            "  - ontar_ptaug_cnn.tar.gz (19929.8 KB)\n",
            "  - ontar_pt_cnn_reg.tar.gz (27686.4 KB)\n",
            "  - ontar_cnn_reg_seq.tar.gz (25832.7 KB)\n",
            "\n",
            "--- Finding and unpacking the pre-trained model ---\n",
            "✅ Found model archive: DeepCRISPR/trained_models/offtar_pt_cnn.tar.gz\n",
            "\n",
            "Unpacking model to: DeepCRISPR/trained_models/model_unpacked/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2788175761.py:87: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tar.extractall(path=model_dir)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model unpacked successfully.\n",
            "\n",
            "Contents of DeepCRISPR/trained_models/model_unpacked/:\n",
            "  - offtar_pt_cnn\n",
            "\n",
            "--- Finding the model's checkpoint files ---\n",
            "✅ Found meta file: DeepCRISPR/trained_models/model_unpacked/offtar_pt_cnn/model.ckpt-off.meta\n",
            "✅ Checkpoint directory: DeepCRISPR/trained_models/model_unpacked/offtar_pt_cnn\n",
            "\n",
            "--- Preparing benchmark data ---\n",
            "Loaded 149483 samples from benchmark data\n",
            "Encoding sequences...\n",
            "  Encoded 10000/149483 samples\n",
            "  Encoded 20000/149483 samples\n",
            "  Encoded 30000/149483 samples\n",
            "  Encoded 40000/149483 samples\n",
            "  Encoded 50000/149483 samples\n",
            "  Encoded 60000/149483 samples\n",
            "  Encoded 70000/149483 samples\n",
            "  Encoded 80000/149483 samples\n",
            "  Encoded 90000/149483 samples\n",
            "  Encoded 100000/149483 samples\n",
            "  Encoded 110000/149483 samples\n",
            "  Encoded 120000/149483 samples\n",
            "  Encoded 130000/149483 samples\n",
            "  Encoded 140000/149483 samples\n",
            "\n",
            "✅ Data prepared successfully\n",
            "   Input shape: (149483, 1, 23, 8)\n",
            "   Labels shape: (149483,)\n",
            "   Expected model input shape: (batch, 1, 23, 8)\n",
            "   Our shape matches: True\n",
            "\n",
            "--- Loading model and running prediction ---\n",
            "Loading model checkpoint...\n",
            "✅ Model loaded successfully\n",
            "\n",
            "Searching for input/output tensors...\n",
            "✅ Found sgRNA input: Placeholder:0\n",
            "✅ Found off-target input: Placeholder_1:0\n",
            "✅ Found prediction output: Softmax:0\n",
            "✅ Found placeholder: Placeholder_2:0 (dtype: <dtype: 'uint8'>)\n",
            "✅ Found placeholder: Placeholder_3:0 (dtype: <dtype: 'bool'>)\n",
            "✅ Found placeholder: Placeholder_4:0 (dtype: <dtype: 'float32'>)\n",
            "✅ Found placeholder: Placeholder_5:0 (dtype: <dtype: 'float32'>)\n",
            "✅ Found placeholder: Placeholder_6:0 (dtype: <dtype: 'float32'>)\n",
            "\n",
            "Running predictions...\n",
            "  Setting 5 placeholder(s) for inference mode\n",
            "    Placeholder_2:0: shape=[None], dtype=<dtype: 'uint8'>, value_shape=(149483,)\n",
            "    Placeholder_3:0: shape=[], dtype=<dtype: 'bool'>, value_shape=()\n",
            "    Placeholder_4:0: shape=[], dtype=<dtype: 'float32'>, value_shape=()\n",
            "    Placeholder_5:0: shape=[], dtype=<dtype: 'float32'>, value_shape=()\n",
            "    Placeholder_6:0: shape=[], dtype=<dtype: 'float32'>, value_shape=()\n",
            "✅ Predictions completed. Shape: (149483, 2)\n",
            "\n",
            "======================================================================\n",
            "🎯 DeepCRISPR Performance Results on Your Benchmark Dataset\n",
            "======================================================================\n",
            "Area Under ROC Curve (AUROC): 0.6056\n",
            "Area Under PR Curve (AUPRC):  0.0480\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CRISOT**"
      ],
      "metadata": {
        "id": "XIxydYpA46Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/bm2-lab/CRISOT.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRPo1s0949cL",
        "outputId": "85181958-b88e-4e7e-f648-d3c6443da0e4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CRISOT'...\n",
            "remote: Enumerating objects: 83, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 83 (delta 15), reused 19 (delta 6), pack-reused 53 (from 1)\u001b[K\n",
            "Receiving objects: 100% (83/83), 157.68 MiB | 22.88 MiB/s, done.\n",
            "Resolving deltas: 100% (27/27), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls CRISOT/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0wrVMgh7psb",
        "outputId": "f7c0f0f6-5457-4d8e-f991-fba13eeefb86"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "crisot_framework.png  CRISOT.py  example  models     script\n",
            "crisot_modules.py     data\t LICENSE  README.md  utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the specific libraries required by CRISOT\n",
        "!pip install xgboost==1.7.3 pandas numpy\n",
        "\n",
        "print(\"✅ Environment for CRISOT is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew1FaTfa7qXB",
        "outputId": "22123d35-8dc0-4dbd-d8bb-43a7429be2dd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xgboost==1.7.3\n",
            "  Downloading xgboost-1.7.3-py3-none-manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost==1.7.3) (1.16.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading xgboost-1.7.3-py3-none-manylinux2014_x86_64.whl (193.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xgboost\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 3.0.5\n",
            "    Uninstalling xgboost-3.0.5:\n",
            "      Successfully uninstalled xgboost-3.0.5\n",
            "Successfully installed xgboost-1.7.3\n",
            "✅ Environment for CRISOT is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find and unzip the model files\n",
        "!unzip -q -o CRISOT/models/crisot-fp_xgb_models.zip -d CRISOT/models/\n",
        "\n",
        "print(\"✅ Pre-trained models for CRISOT unzipped successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "701g-9Y181XU",
        "outputId": "71566c00-db30-40d7-d460-a3e6e251e162"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open CRISOT/models/crisot-fp_xgb_models.zip, CRISOT/models/crisot-fp_xgb_models.zip.zip or CRISOT/models/crisot-fp_xgb_models.zip.ZIP.\n",
            "✅ Pre-trained models for CRISOT unzipped successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "print(\"Loading our benchmark dataset...\")\n",
        "df = pd.read_csv('cleaned_benchmark_data.csv')\n",
        "\n",
        "# Rename the columns to match the required format for CRISOT\n",
        "crisot_input_df = df.rename(columns={\n",
        "    'sgRNA': 'On',\n",
        "    'off_target': 'Off'\n",
        "})\n",
        "\n",
        "# Save the formatted data to a new CSV file\n",
        "crisot_input_df.to_csv('crisot_benchmark_input.csv', index=False)\n",
        "\n",
        "print(\"✅ Benchmark data formatted for CRISOT and saved to 'crisot_benchmark_input.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3OcP14M83LT",
        "outputId": "cf3628bd-422e-414a-c3c5-a8e7110cf353"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading our benchmark dataset...\n",
            "✅ Benchmark data formatted for CRISOT and saved to 'crisot_benchmark_input.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 5 lines of the output file, including the header\n",
        "!head -n 5 crisot_output.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gbSUSnqBzL0",
        "outputId": "684a4091-c0bf-49e1-bdb4-f458abcb3d0b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On,Off,label,CRISOT_Score\n",
            "GCAGCCAGTACAGCTCACCATGG,GCTGCCAGTACAGGCTCCCCCTC,0,0.19869676422952098\n",
            "GCAGCCAGTACAGCTCACCATGG,GCTGCCAGTACAGGCTCCCCCTC,0,0.19869676422952098\n",
            "GCTAGAGTCACAAGTCCCACAGG,TACTAGAGTGACAAGTCACACAA,0,0.0\n",
            "GCAGCCAGTACAGCTCACCATGG,ACAGCGAGTACAAGCTCATCATG,0,0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "print(\"--- Calculating final performance metrics ---\")\n",
        "try:\n",
        "    # Load the prediction results generated by the script\n",
        "    results_df = pd.read_csv('crisot_output.csv')\n",
        "\n",
        "    # Load your original benchmark data to get the ground truth labels\n",
        "    ground_truth_df = pd.read_csv('cleaned_benchmark_data.csv')\n",
        "\n",
        "    # Extract the true labels and the predicted scores using the CORRECT column name\n",
        "    y_test = ground_truth_df['label']\n",
        "    y_scores = results_df['CRISOT_Score'] # Corrected from 'CRISOT-Score'\n",
        "\n",
        "    # Calculate the performance metrics\n",
        "    auroc = roc_auc_score(y_test, y_scores)\n",
        "    auprc = average_precision_score(y_test, y_scores)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"🎯 CRISOT Performance Results on Your Benchmark Dataset\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Area Under ROC Curve (AUROC): {auroc:.4f}\")\n",
        "    print(f\"Area Under PR Curve (AUPRC):  {auprc:.4f}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"❌ Error: 'crisot_output.csv' was not created. Please review the prediction logs above for errors.\")\n",
        "except KeyError:\n",
        "    print(\"❌ Error: Could not find the 'CRISOT_Score' column. Please check the output of '!head crisot_output.csv' again.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during evaluation: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEMR1P0tB4U7",
        "outputId": "5956fbeb-35ab-412f-812d-b38f3141a55b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Calculating final performance metrics ---\n",
            "\n",
            "======================================================================\n",
            "🎯 CRISOT Performance Results on Your Benchmark Dataset\n",
            "======================================================================\n",
            "Area Under ROC Curve (AUROC): 0.9888\n",
            "Area Under PR Curve (AUPRC):  0.0723\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **CRISPR IP** *italicized text*"
      ],
      "metadata": {
        "id": "PE7yI3qkFSZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/BioinfoVirgo/CRISPR-IP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IycxlcVJCBTE",
        "outputId": "4def9b92-618e-4201-9a5c-f89868ce5498"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CRISPR-IP'...\n",
            "remote: Enumerating objects: 51, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 51 (delta 16), reused 45 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (51/51), 19.55 MiB | 4.88 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n",
            "Updating files: 100% (20/20), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install modern, compatible versions of the required libraries\n",
        "!pip install tensorflow pandas scikit-learn\n",
        "\n",
        "print(\"✅ Environment for CRISPR-IP is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nC7K-TqH0wc",
        "outputId": "b4d7bc7e-38ab-450b-e6a5-e50e23db047e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "✅ Environment for CRISPR-IP is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"--- Preparing benchmark data for CRISPR-IP ---\")\n",
        "\n",
        "# --- Encoding logic from the encoding.py file ---\n",
        "encoded_dict = {'A': [1, 0, 0, 0], 'T': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'C': [0, 0, 0, 1], '_': [0, 0, 0, 0], '-': [0, 0, 0, 0]}\n",
        "pos_dict = {'A':1, 'T':2, 'G':3, 'C':4, '_':5, '-':5}\n",
        "\n",
        "def encode_sequence_pair(sgRNA_seq, off_target_seq):\n",
        "    tlen = 24\n",
        "    # Pad sequences to 24 characters\n",
        "    target_seq = \"_\"*(tlen-len(sgRNA_seq)) + sgRNA_seq\n",
        "    off_target_seq = \"_\"*(tlen-len(off_target_seq)) + off_target_seq\n",
        "\n",
        "    target_seq_code = np.array([encoded_dict[base] for base in list(target_seq)])\n",
        "    off_target_seq_code = np.array([encoded_dict[base] for base in list(off_target_seq)])\n",
        "\n",
        "    on_off_dim6_codes = []\n",
        "    for i in range(tlen):\n",
        "        diff_code = np.bitwise_or(target_seq_code[i], off_target_seq_code[i])\n",
        "        dir_code = np.zeros(2)\n",
        "        if pos_dict[target_seq[i]] == pos_dict[off_target_seq[i]]:\n",
        "            diff_code = diff_code * -1\n",
        "            dir_code[0] = 1\n",
        "            dir_code[1] = 1\n",
        "        elif pos_dict[target_seq[i]] < pos_dict[off_target_seq[i]]:\n",
        "            dir_code[0] = 1\n",
        "        elif pos_dict[target_seq[i]] > pos_dict[off_target_seq[i]]:\n",
        "            dir_code[1] = 1\n",
        "\n",
        "        on_off_dim6_codes.append(np.concatenate((diff_code, dir_code)))\n",
        "\n",
        "    on_off_dim6_codes = np.array(on_off_dim6_codes)\n",
        "    isPAM = np.zeros((24, 1))\n",
        "    isPAM[-3:, :] = 1\n",
        "    on_off_code = np.concatenate((on_off_dim6_codes, isPAM), axis=1)\n",
        "    return on_off_code\n",
        "# --- End of encoding logic ---\n",
        "\n",
        "# Load your benchmark data\n",
        "df = pd.read_csv('cleaned_benchmark_data.csv')\n",
        "\n",
        "encoded_features = []\n",
        "labels = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    sgRNA_seq = row['sgRNA']\n",
        "    off_target_seq = row['off_target']\n",
        "\n",
        "    # Encode the pair and add it to our list\n",
        "    encoded_pair = encode_sequence_pair(sgRNA_seq, off_target_seq)\n",
        "    encoded_features.append(encoded_pair)\n",
        "    labels.append(row['label'])\n",
        "\n",
        "# Convert to final NumPy arrays\n",
        "X = np.array(encoded_features)\n",
        "y = np.array(labels)\n",
        "\n",
        "print(\"\\n✅ Preprocessing complete!\")\n",
        "print(f\"Shape of encoded features: {X.shape}\")\n",
        "print(f\"Shape of Labels: {y.shape}\")\n",
        "\n",
        "# Save the processed data\n",
        "np.savez('crispr_ip_benchmark_encoded.npz', X=X, y=y)\n",
        "print(\"\\n💾 Saved preprocessed data to 'crispr_ip_benchmark_encoded.npz'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Dw4xGUSIEMM",
        "outputId": "8bf61d65-5fab-4368-a198-6fbfb6263719"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Preparing benchmark data for CRISPR-IP ---\n",
            "\n",
            "✅ Preprocessing complete!\n",
            "Shape of encoded features: (149483, 24, 7)\n",
            "Shape of Labels: (149483,)\n",
            "\n",
            "💾 Saved preprocessed data to 'crispr_ip_benchmark_encoded.npz'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls CRISPR-IP/example_saved/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQ3eO_OVImn7",
        "outputId": "10022b81-0d6b-42ed-fac9-066db8a3eef0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example+crispr_ip.h5\t    example-test-data.csv\n",
            "example-predict-result.csv  example-train-data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls CRISPR-IP/codes/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kLd4p-9MR6M",
        "outputId": "d32c2f29-8c2d-402f-bd77-bd0150bd47f8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CRISPR_IP.py  encoding.py  __init__.py\t__pycache__\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat CRISPR-IP/codes/CRISPR_IP.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4q6hIuT3NcWj",
        "outputId": "d90bcc5b-8c75-405f-bb0a-e106126f05e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import os\n",
            "import numpy as np\n",
            "import tensorflow as tf\n",
            "from tensorflow.keras.models import model_from_json, load_model\n",
            "from tensorflow.keras.models import Model\n",
            "from tensorflow.keras.layers import Attention, Dense, Conv2D, Bidirectional, LSTM, Flatten, Input, Activation, Reshape, Dropout, Concatenate, AveragePooling1D, MaxPool1D, BatchNormalization, Attention, GlobalAveragePooling1D, GlobalMaxPool1D, GRU, AdditiveAttention, AlphaDropout, LeakyReLU\n",
            "from tensorflow.keras.initializers import VarianceScaling\n",
            "from tensorflow.keras.utils import to_categorical\n",
            "\n",
            "def transformIO(xtrain, xtest, ytrain, ytest, seq_len , coding_dim, num_classes):\n",
            "    xtrain = xtrain.reshape(xtrain.shape[0], 1, seq_len, coding_dim)\n",
            "    xtest = xtest.reshape(xtest.shape[0], 1, seq_len, coding_dim)\n",
            "    input_shape = (1, seq_len, coding_dim)\n",
            "    xtrain = xtrain.astype('float32')\n",
            "    xtest = xtest.astype('float32')\n",
            "    print('xtrain shape:', xtrain.shape)\n",
            "    print(xtrain.shape[0], 'train samples')\n",
            "    print(xtest.shape[0], 'test samples')\n",
            "\n",
            "    ytrain = to_categorical(ytrain, num_classes)\n",
            "    ytest = to_categorical(ytest, num_classes)\n",
            "    return xtrain, xtest, ytrain, ytest, input_shape\n",
            "\n",
            "def crispr_ip(xtrain, ytrain, xtest, ytest, input_shape, num_classes, batch_size, epochs, callbacks, saved_prefix, retrain=False):\n",
            "    if retrain or not os.path.exists('{}+crispr_ip.h5'.format(saved_prefix)):\n",
            "        initializer = VarianceScaling(mode='fan_avg', distribution='uniform')\n",
            "        input_value = Input(shape=input_shape)\n",
            "        conv_1_output = Conv2D(60, (1,input_shape[-1]), padding='valid', data_format='channels_first', kernel_initializer=initializer)(input_value)\n",
            "        conv_1_output_reshape = Reshape(tuple([x for x in conv_1_output.shape.as_list() if x != 1 and x is not None]))(conv_1_output)\n",
            "        conv_1_output_reshape2 = tf.transpose(conv_1_output_reshape, perm=[0,2,1])\n",
            "        conv_1_output_reshape_average = AveragePooling1D(data_format='channels_first')(conv_1_output_reshape2)\n",
            "        conv_1_output_reshape_max = MaxPool1D(data_format='channels_first')(conv_1_output_reshape2)\n",
            "        bidirectional_1_output = Bidirectional(LSTM(30, return_sequences=True, dropout=0.25, kernel_initializer=initializer))(Concatenate(axis=-1)([conv_1_output_reshape_average, conv_1_output_reshape_max]))\n",
            "        attention_1_output = Attention()([bidirectional_1_output, bidirectional_1_output])\n",
            "        average_1_output = GlobalAveragePooling1D(data_format='channels_last')(attention_1_output)\n",
            "        max_1_output = GlobalMaxPool1D(data_format='channels_last')(attention_1_output)\n",
            "        concat_output = Concatenate(axis=-1)([average_1_output, max_1_output])\n",
            "        flatten_output = Flatten()(concat_output)\n",
            "        linear_1_output = BatchNormalization()(Dense(200, activation='relu', kernel_initializer=initializer)(flatten_output))\n",
            "        linear_2_output = Dense(100, activation='relu', kernel_initializer=initializer)(linear_1_output)\n",
            "        linear_2_output_dropout = Dropout(0.9)(linear_2_output)\n",
            "        linear_3_output = Dense(num_classes, activation='softmax', kernel_initializer=initializer)(linear_2_output_dropout)\n",
            "        model = Model(input_value, linear_3_output)\n",
            "        model.compile(tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
            "        history_model = model.fit(\n",
            "            xtrain, ytrain,\n",
            "            batch_size=batch_size,\n",
            "            epochs=epochs,\n",
            "            verbose=1,\n",
            "            validation_data=(xtest, ytest),\n",
            "            callbacks=callbacks\n",
            "        )\n",
            "        model.save('{}+crispr_ip.h5'.format(saved_prefix))\n",
            "    model = load_model('{}+crispr_ip.h5'.format(saved_prefix))\n",
            "    return model"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix, classification_report\n",
        "\n",
        "# === STEP 1: INSTALL LIBRARIES ===\n",
        "print(\"--- Installing libraries ---\")\n",
        "!pip install tensorflow pandas scikit-learn -q\n",
        "print(\"✅ Libraries installed.\\n\")\n",
        "\n",
        "# === STEP 2: CLONE REPOSITORY ===\n",
        "print(\"--- Cloning CRISPR-IP repository ---\")\n",
        "if not os.path.exists('CRISPR-IP'):\n",
        "    !git clone https://github.com/BioinfoVirgo/CRISPR-IP.git\n",
        "else:\n",
        "    print(\"✅ Repository already exists.\\n\")\n",
        "\n",
        "# === STEP 3: PREPARE DATA WITH ORIGINAL ENCODING ===\n",
        "print(\"--- Preparing benchmark data with ORIGINAL CRISPR-IP encoding ---\")\n",
        "if not os.path.exists('cleaned_benchmark_data.csv'):\n",
        "    print(\"❌ Critical Error: 'cleaned_benchmark_data.csv' not found. Please upload it first.\")\n",
        "    exit()\n",
        "\n",
        "df = pd.read_csv('cleaned_benchmark_data.csv')\n",
        "\n",
        "# === ORIGINAL ENCODING FROM CRISPR-IP ===\n",
        "encoded_dict = {'A': [1,0,0,0], 'T': [0,1,0,0], 'G': [0,0,1,0], 'C': [0,0,0,1], '_': [0,0,0,0], '-': [0,0,0,0]}\n",
        "pos_dict = {'A':1, 'T':2, 'G':3, 'C':4, '_':5, '-':5}\n",
        "\n",
        "def my_encode_on_off_dim(target_seq, off_target_seq):\n",
        "    \"\"\"Original encoding function from CRISPR-IP\"\"\"\n",
        "    tlen = 24\n",
        "    # IMPORTANT: Uses \"-\" for padding, not \"_\"\n",
        "    target_seq = \"-\" * (tlen - len(target_seq)) + target_seq\n",
        "    off_target_seq = \"-\" * (tlen - len(off_target_seq)) + off_target_seq\n",
        "\n",
        "    target_seq_code = np.array([encoded_dict[base] for base in list(target_seq)])\n",
        "    off_target_seq_code = np.array([encoded_dict[base] for base in list(off_target_seq)])\n",
        "\n",
        "    on_off_dim6_codes = []\n",
        "    for i in range(len(target_seq)):\n",
        "        diff_code = np.bitwise_or(target_seq_code[i], off_target_seq_code[i])\n",
        "        dir_code = np.zeros(2)\n",
        "\n",
        "        if pos_dict[target_seq[i]] == pos_dict[off_target_seq[i]]:\n",
        "            diff_code = diff_code * -1\n",
        "            dir_code[0] = 1\n",
        "            dir_code[1] = 1\n",
        "        elif pos_dict[target_seq[i]] < pos_dict[off_target_seq[i]]:\n",
        "            dir_code[0] = 1\n",
        "        elif pos_dict[target_seq[i]] > pos_dict[off_target_seq[i]]:\n",
        "            dir_code[1] = 1\n",
        "        else:\n",
        "            raise Exception(\"Invalid seq!\", target_seq, off_target_seq)\n",
        "\n",
        "        on_off_dim6_codes.append(np.concatenate((diff_code, dir_code)))\n",
        "\n",
        "    on_off_dim6_codes = np.array(on_off_dim6_codes)\n",
        "    isPAM = np.zeros((24, 1))\n",
        "    isPAM[-3:, :] = 1\n",
        "    on_off_code = np.concatenate((on_off_dim6_codes, isPAM), axis=1)\n",
        "    return on_off_code\n",
        "\n",
        "# Encode data\n",
        "print(\"Encoding sequences...\")\n",
        "encoded_features, labels = [], []\n",
        "for _, row in df.iterrows():\n",
        "    encoded_features.append(my_encode_on_off_dim(row['sgRNA'], row['off_target']))\n",
        "    labels.append(row['label'])\n",
        "\n",
        "X_test = np.array(encoded_features, dtype=np.float32).reshape(-1, 1, 24, 7)\n",
        "y_test = np.array(labels)\n",
        "print(f\"✅ Data encoded. Shape: {X_test.shape}\\n\")\n",
        "\n",
        "# === STEP 4: BUILD MODEL WITH ORIGINAL ARCHITECTURE ===\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, Reshape, AveragePooling1D, MaxPool1D, Bidirectional, LSTM, Attention, GlobalAveragePooling1D, GlobalMaxPool1D, Flatten, BatchNormalization, Dense, Dropout, Concatenate, Permute\n",
        "from tensorflow.keras.initializers import VarianceScaling\n",
        "\n",
        "print(\"--- Building model with ORIGINAL CRISPR-IP architecture ---\")\n",
        "input_shape = (1, 24, 7)\n",
        "initializer = VarianceScaling(mode='fan_avg', distribution='uniform')\n",
        "\n",
        "input_value = Input(shape=input_shape)\n",
        "conv_1_output = Conv2D(60, (1, 7), padding='valid', data_format='channels_first', kernel_initializer=initializer)(input_value)\n",
        "\n",
        "# After Conv2D: shape is (None, 60, 1, 24)\n",
        "# Reshape to (60, 24) - keeping channels first\n",
        "conv_1_output_reshape = Reshape((60, 24))(conv_1_output)\n",
        "\n",
        "# Transpose to (24, 60) - swapping dimensions\n",
        "conv_1_output_reshape2 = Permute((2, 1))(conv_1_output_reshape)\n",
        "\n",
        "# Now with channels_first pooling on (24, 60):\n",
        "# This treats 24 as channels and pools along the 60 dimension\n",
        "# Result: (24, 30) for each pooling operation\n",
        "conv_1_output_reshape_average = AveragePooling1D(pool_size=2, data_format='channels_first')(conv_1_output_reshape2)\n",
        "conv_1_output_reshape_max = MaxPool1D(pool_size=2, data_format='channels_first')(conv_1_output_reshape2)\n",
        "\n",
        "# Concatenate along last axis: (24, 30) + (24, 30) = (24, 60)\n",
        "# This gives us 60 input features to the LSTM, which matches the saved weights!\n",
        "\n",
        "# Concatenate and continue\n",
        "bidirectional_1_output = Bidirectional(LSTM(30, return_sequences=True, dropout=0.25, kernel_initializer=initializer))(\n",
        "    Concatenate(axis=-1)([conv_1_output_reshape_average, conv_1_output_reshape_max])\n",
        ")\n",
        "\n",
        "attention_1_output = Attention()([bidirectional_1_output, bidirectional_1_output])\n",
        "average_1_output = GlobalAveragePooling1D(data_format='channels_last')(attention_1_output)\n",
        "max_1_output = GlobalMaxPool1D(data_format='channels_last')(attention_1_output)\n",
        "concat_output = Concatenate(axis=-1)([average_1_output, max_1_output])\n",
        "flatten_output = Flatten()(concat_output)\n",
        "linear_1_output = BatchNormalization()(Dense(200, activation='relu', kernel_initializer=initializer)(flatten_output))\n",
        "linear_2_output = Dense(100, activation='relu', kernel_initializer=initializer)(linear_1_output)\n",
        "linear_2_output_dropout = Dropout(0.9)(linear_2_output)\n",
        "linear_3_output = Dense(2, activation='softmax', kernel_initializer=initializer)(linear_2_output_dropout)\n",
        "\n",
        "model = Model(input_value, linear_3_output)\n",
        "print(\"✅ Model architecture built.\\n\")\n",
        "\n",
        "# === STEP 5: LOAD WEIGHTS AND PREDICT ===\n",
        "model_path = 'CRISPR-IP/example_saved/example+crispr_ip.h5'\n",
        "print(f\"--- Loading weights from {model_path} ---\")\n",
        "model.load_weights(model_path)\n",
        "print(\"✅ Weights loaded successfully.\\n\")\n",
        "\n",
        "print(\"--- Making predictions ---\")\n",
        "y_pred_probs = model.predict(X_test, batch_size=1024)\n",
        "y_scores = y_pred_probs[:, 1]\n",
        "y_pred_labels = (y_scores >= 0.5).astype(int)\n",
        "\n",
        "# === STEP 6: EVALUATION ===\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🎯 CRISPR-IP PERFORMANCE WITH CORRECT ENCODING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "auroc = roc_auc_score(y_test, y_scores)\n",
        "auprc = average_precision_score(y_test, y_scores)\n",
        "\n",
        "print(f\"AUROC: {auroc:.4f}\")\n",
        "print(f\"AUPRC: {auprc:.4f}\")\n",
        "\n",
        "print(\"\\n📊 Confusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred_labels)\n",
        "print(cm)\n",
        "\n",
        "print(\"\\n📋 Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_labels, target_names=['No Activity (0)', 'Activity (1)']))\n",
        "\n",
        "print(\"\\n📈 Prediction Score Statistics:\")\n",
        "print(f\"Min:  {y_scores.min():.6f}\")\n",
        "print(f\"Max:  {y_scores.max():.6f}\")\n",
        "print(f\"Mean: {y_scores.mean():.6f}\")\n",
        "print(f\"Median: {np.median(y_scores):.6f}\")\n",
        "\n",
        "if (y_test == 1).sum() > 0:\n",
        "    positive_scores = y_scores[y_test == 1]\n",
        "    print(f\"\\n🎯 TRUE POSITIVE scores (label=1):\")\n",
        "    print(f\"  Mean: {positive_scores.mean():.6f}\")\n",
        "    print(f\"  Median: {np.median(positive_scores):.6f}\")\n",
        "\n",
        "if (y_test == 0).sum() > 0:\n",
        "    negative_scores = y_scores[y_test == 0]\n",
        "    print(f\"\\n🎯 TRUE NEGATIVE scores (label=0):\")\n",
        "    print(f\"  Mean: {negative_scores.mean():.6f}\")\n",
        "    print(f\"  Median: {np.median(negative_scores):.6f}\")\n",
        "\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6PSuO3iaR5E",
        "outputId": "4d852447-d23f-43d7-c1b0-f21717105057"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Installing libraries ---\n",
            "✅ Libraries installed.\n",
            "\n",
            "--- Cloning CRISPR-IP repository ---\n",
            "✅ Repository already exists.\n",
            "\n",
            "--- Preparing benchmark data with ORIGINAL CRISPR-IP encoding ---\n",
            "Encoding sequences...\n",
            "✅ Data encoded. Shape: (149483, 1, 24, 7)\n",
            "\n",
            "--- Building model with ORIGINAL CRISPR-IP architecture ---\n",
            "✅ Model architecture built.\n",
            "\n",
            "--- Loading weights from CRISPR-IP/example_saved/example+crispr_ip.h5 ---\n",
            "✅ Weights loaded successfully.\n",
            "\n",
            "--- Making predictions ---\n",
            "\u001b[1m146/146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
            "\n",
            "======================================================================\n",
            "🎯 CRISPR-IP PERFORMANCE WITH CORRECT ENCODING\n",
            "======================================================================\n",
            "AUROC: 0.7812\n",
            "AUPRC: 0.0009\n",
            "\n",
            "📊 Confusion Matrix:\n",
            "[[130730  18711]\n",
            " [    13     29]]\n",
            "\n",
            "📋 Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "No Activity (0)       1.00      0.87      0.93    149441\n",
            "   Activity (1)       0.00      0.69      0.00        42\n",
            "\n",
            "       accuracy                           0.87    149483\n",
            "      macro avg       0.50      0.78      0.47    149483\n",
            "   weighted avg       1.00      0.87      0.93    149483\n",
            "\n",
            "\n",
            "📈 Prediction Score Statistics:\n",
            "Min:  0.000000\n",
            "Max:  1.000000\n",
            "Mean: 0.130958\n",
            "Median: 0.000008\n",
            "\n",
            "🎯 TRUE POSITIVE scores (label=1):\n",
            "  Mean: 0.700707\n",
            "  Median: 0.987733\n",
            "\n",
            "🎯 TRUE NEGATIVE scores (label=0):\n",
            "  Mean: 0.130798\n",
            "  Median: 0.000008\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}